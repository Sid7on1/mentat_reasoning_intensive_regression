{
  "agent_id": "coder4",
  "task_id": "task_5",
  "files": [
    {
      "filename": "notebooks/analysis.ipynb",
      "purpose": "Jupyter notebook for interactive analysis of results and visualization of prediction patterns",
      "priority": "low",
      "dependencies": [
        "jupyter",
        "matplotlib",
        "seaborn",
        "pandas",
        "numpy"
      ],
      "key_functions": [
        "load_results",
        "plot_quantization_analysis",
        "visualize_prompt_evolution",
        "compare_model_performance"
      ],
      "estimated_lines": 150,
      "complexity": "low"
    }
  ],
  "project_info": {
    "project_name": "MENTAT_Reasoning_Intensive_Regression",
    "project_type": "nlp",
    "description": "Implementation of MENTAT (Mistake-Aware prompt Evolver with Neural Training And Testing), a lightweight method for reasoning-intensive regression tasks that combines iterative prompt optimization with neural ensemble learning. The system addresses the challenge of producing precise numerical predictions from text requiring deep reasoning, such as mathematical error detection, pairwise RAG comparison, and essay grading.",
    "key_algorithms": [
      "MENTAT_prompt_optimization",
      "neural_ensemble_aggregation",
      "batch_reflective_error_analysis",
      "multi_rollout_consensus",
      "CCC_NMSE_combined_loss"
    ],
    "main_libraries": [
      "torch",
      "transformers",
      "numpy",
      "pandas",
      "scikit_learn",
      "datasets",
      "openai",
      "tqdm",
      "matplotlib",
      "seaborn"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.21762v1_Reasoning-Intensive-Regression.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nREASONING -INTENSIVE REGRESSION\nDiane Tchuindjo\nOperations Research Center\nMIT\ndianetc@mit.eduOmar Khattab\nEECS & CSAIL\nMIT\nokhattab@mit.edu\nABSTRACT\nAI researchers and practitioners increasingly apply large language models (LLMs)\nto what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical\nproperties from text. Unlike standard language regression tasks, e.g. for sentiment\nor similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring\nor domain-specific retrieval, where much deeper analysis of text is required while\nonly limited task-specific training data and computation are available. We cast three\nrealistic problems as RiR tasks to establish an initial benchmark, and use that to test\nour hypothesis that prompting frozen LLMs and finetuning Transformer encoders\nvia gradient descent will both often struggle in RiR. We then propose MENTAT , a\nsimple and lightweight method that combines batch-reflective prompt optimization\nwith neural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.\n1 I NTRODUCTION\n0 2 4 6 8 10\nGround Truth0246810Predictions\nCCC : 0.008\nNMSE : 1.014\nFinetuning NeoBERT\n0 2 4 6 8 10\nGround Truth0246810Predictions\nCCC : 0.685\nNMSE : 0.809\nDetailed Prompt for GPT5\n0 2 4 6 8 10\nGround Truth0246810Predictions\nCCC : 0.792\nNMSE : 0.402\nMENTAT w. GPT5 (Ours)\n0246810121416\nDensity\n0246810\nDensity\n01234567\nDensity\nFigure 1: On regression for detecting the first math error, finetuning a NeoBERT model collapses to\nmean predictions (CCC = 0.01). Meanwhile, detailed (human-crafted) prompting achieves reasonable\nconcordance (CCC = 0.69) but exhibits coarse and imprecise prediction behavior (the dense horizontal\nlines and near-random NMSE). MENTAT \u2019s performance illustrates how RiR problems benefit from\ncombining deep reasoning capabilities with precise numerical predictions.\nDespite fast progress in adapting large language models (LLMs) for building downstream AI systems,\nlightweight methods for adapting LLMs to even standard natural-language regression tasks remain\nsurprisingly elusive (Lukasik et al., 2024b;a; Tang et al., 2024; Song et al., 2025; Song & Bahri,\n2025). These tasks, like sentiment analysis, semantic similarity, and document ranking, involve\npredicting a score y\u2208Rfrom a natural-language string. Surprisingly, on these problems, applying\nstraightforward supervised learning to pretrained Transformer encoders such as BERT (Devlin et al.,\n2019) has been shown to perform competitively with much larger decoder-only LLMs (Lukasik et al.,\n2024a), even with sophisticated fine-tuning methods.\nWe investigate what we call Reasoning-Intensive Regression (RiR), a fuzzy but growing subset\nof natural-language regression in which processing the text in each instance demands sequential\ndeduction or deep analysis, rather than shallow identification of features . Unlike simpler regression\ntasks, RiR problems call for explicit step-by-step problem decomposition or reasoning , where the\n1arXiv:2508.21762v1  [cs.CL]  29 Aug 2025\n\n--- Page 2 ---\nsystem produces intermediate sequences of steps like tokens \u27e8r1, ..., r t\u27e9 \u2208\u03a3\u2217before committing to a\nprediction (Merrill & Sabharwal, 2024). See Figure 2 for a breakdown of regression problems into\nthree levels of complexity: feature-based, semantic analysis, and reasoning-intensive, inspired by Su\net al. (2025)\u2019s analysis of retrieval tasks.\nThese types of applications are emerging rapidly in both research and practice, e.g. to produce scores\nfor ad-hoc applications that process customer calls, student essays, rubric-based LLM generation,\nor instruction-based query\u2013document relevance (MacDonald, 2024; Es et al., 2024; Su et al., 2025;\nThakur et al., 2025). In parallel, the same scoring paradigm is being scaled in recent efforts toward\ngeneral-purpose chain-of-thought reward models (Kimi Team, 2025; Ankner et al., 2024a), but these\ntypically assume orders-of-magnitude more labels and compute (e.g., hundreds of thousands of labels\nin K2) than the lightweight application-specific regimes that are far more common in the long tail.\nWe establish an initial benchmark for RiR by casting three realistic tasks as regression problems that\ndemand varying levels of reasoning: predicting the proportion of a long mathematical deduction up to\nthe first erroneous statement, predicting the degree to which the response of one Retrieval-Augmented\nGeneration (RAG) system is better than another, and grading student essays on supplied topics. We\nthen identify two practical constraints of downstream applications of RiR: these applications tend to\noffer only (very) small training sets and have room only for accessible and lightweight computations\nlike LLM inference, lightweight forms of LLM prompt optimization, and finetuning medium-sized\nneural networks such as small Transformers, but not, say, large-scale reinforcement learning for large\nlanguage models (DeepSeek-AI, 2025; Kimi Team, 2025).\nWe ask: Are there effective methods that are data- and compute-efficient for tackling ad-hoc reasoning-\nintensive regression problems? We hypothesize that what makes RiR problems especially challenging\nis that they combine the reasoning need for deep analysis of each individual task instance with\nthe regression challenge of learning to produce precise, calibrated, and well-ranked scores from\nvery little data. As illustrated in Figure 1, standard prompt engineering techniques struggle with\nthe high precision needed for learning to approximate a statistical distribution, while approaches\nthat bypass LLM-based reasoning, e.g., training small Transformer encoders, often fail to truly\nlearn RiR problems and instead seek to \u201chack\u201d the regression loss function by finding degenerate\napproximations (e.g., collapsing to a small range of scores).\nWe propose the Mistake-Aware prompt Evolver with Neural Training AndTesting ( MENTAT ), a\nsimple and lightweight method that combines iterative prompt optimization with neural regression.\nRather than relying on LLMs to produce precise numerical predictions directly, which often results in\nbrittle outputs, MENTAT uses an iterative error-driven prompt evolution process. Starting with even just\na very basic prompt, the LLM analyzes its own prediction errors in large batches , identifies patterns\nof its poor performance, and then refines the prompt based on that. After just a few iterations, the\nLLM-discovered prompt is used to train a simple aggregation system in which the LLM produces\nmultiple rollouts and an MLP, trained accordingly, is used to reduce the rollouts to a final prediction.\nMENTAT delivers consistent improvements in quality, but nonetheless leaves large headroom on many\nof the RiR settings we define.\nThe remainder of the study is structured as follows: Section 2 describes how we translate three prob-\nlems into RiR tasks and Section 3 introduces MENTAT . Section 4 presents our evaluation methodology,\nincluding the details of our baselines, and the results. The paper concludes with Sections 5 and 6,\nwhich synthesize our findings and discuss implications for future research. An extended related work\ndiscussion is in Appendix A.\n2 B ENCHMARKING RIR\nWe collect three tasks for Reasoning-Intensive Regression of varying degrees of reasoning intensity.\nRefer to Figure 3 for the dataset distributions.\n\u2022Mathematical Error Detection requires precise logical reasoning and stepwise analysis,\nwhile also stressing the fact that LLMs are known to struggle with precisely estimating\nsimple properties like text length.\n\u2022Pairwise RAG Comparison demands nuanced judgment on a comparative scale and\ndeducing a model of that from only little data.\n2\n\n--- Page 3 ---\nHouse Price Prediction\nHouse Size: 1100 sqft\nBedrooms: 3\nBathrooms: 2\nLot Size: 2000 sqftStandard Regression Methods\nVarious techniques, such as linear re-\ngression, can lead to a prediction. Ex-\nample: 0.3\u00d71100 + 15 \u00d73 + 10 \u00d72 +\n0.05\u00d72000 + 50 = $540 ,000\nEssay Grading\nPrompt : Distance Learning\nEssay : \u201cI think that students would benefit from distance learning\nbecause they won\u2019t have to change. . . \u201d\nAge: 12Simple Semantic Understanding\nSimple semantic analysis based on co-\nherence, grammar and style can lead to\nhuman-like scores: 3.5/5.0\nDetect Math Errors\nProblem : Find all ordered pairs (a, b)of positive integers such that\n2a+ 1divides 3b\u22121and 2b+ 1divides 3a\u22121.\nCandidate Solution : To find all ordered pairs (a,b), we can start by\nsetting up two equations based on the conditions. Let 2a+ 1 = x...\nPairwise RAG Comparison\nQuery : Which gene-defect causes the vel-blood type?\nReference Answer : A 17 nucleotide deletion.\nSystem Answer : The gene defect that causes the Vel- blood type is\na homozygous 17 base pair deletion in the SMIM1 gene...Deep Reasoning:\nUse deep reasoning to find where the\nnormalized candidate solution is incor-\nrect. For this example, it is 5.178\nDeep Reasoning:\nUse deep reasoning to compare how\nmuch the system answer is preferred to\nthe reference answer. In this example,\nthe score is 1.33, indicating the system\nanswer is slightly more preferred to the\nreference one.Level 1: Feature-Based\nLevel 2: Semantic Analysis\nLevel 3: Reasoning-IntensiveFigure 2: Inspired by Su et al. (2025)\u2019s analysis of retrieval tasks, we break down text-based regression\nproblems into three, informal complexity levels. Level 1 tasks use simple feature-based inputs (house\nsize, bedrooms for price prediction). Text-to-text regression achieves strong Level 1 performance with\nrich datasets (Akhauri et al., 2025). Level 2 tasks require moderate semantic understanding (sentiment\nanalysis, reward modeling) but are easy for supervised-learning over a pretrained Transformer. Level\n3 represents Reasoning-Intensive Regression (RiR), requiring deep sequential reasoning, which is the\nfocus of this work.\n\u2022Essay Grading serves as a reference point, requiring semantic understanding where encoder-\nonly models, like BERT, might already perform well with a reasonable amount of finetuning\ndata.\n0 2 4 6 8 10\nScore020406080100120FrequencyDistribution/uni00A0of/uni00A0Math/uni00A0Error/uni00A0Regression/uni00A0Scores\nMean:/uni00A04.62\nMedian:/uni00A04.47\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0\nScore0100200300400500FrequencyDistribution/uni00A0of/uni00A0Pairwise/uni00A0RAG/uni00A0Comparison/uni00A0Scores\nMean:/uni00A0/uni00AD0.10\nMedian:/uni00A0/uni00AD0.33\n1 2 3 4 5\nScore025050075010001250FrequencyDistribution/uni00A0of/uni00A0Overall/uni00A0Essay/uni00A0Scores\nMean:/uni00A03.11\nMedian:/uni00A03.00\nFigure 3: Ground-truth score distributions for mathematical error detection (spread capturing solution\nprogression), pairwise RAG comparison (narrow distribution around averaged judgments), and essay\ngrading (tight clustering characteristic of qualitative assessments).\n2.1 R EGRESSION METRICS\nNormalized Mean Square Error (NMSE) is a common metric for reporting regression performance:\nnP\ni(yi\u2212\u02c6yi)2/nP\ni(yi\u2212\u00afy)2, where nis the size of the dataset, \u02c6yiis a prediction, yithe corresponding\nground truth value, and \u00afyis the mean of the set.\nHowever, distance-based metrics are inadequate for typical RiR problems, as it can be easy for RiR\nsystems to artificially lower their NMSE simply by avoiding \u201crisky\u201d predictions at the extremes,\nwithout good calibration. This can be seen in Figure 1 earlier, particularly in comparing the fine-tuned\nNeoBERT model (Breton et al., 2025) against detailed (human-crafted) prompting. If we were to rely\non NMSE, detailed prompting for gpt-5 would not appear to substantially outperform NeoBERT\n3\n\n--- Page 4 ---\n(0.81vs.1.01), and this gap would be even reversed for weaker LLMs. However, examining the\ndistribution of predictions reveals that NeoBERT simply \u201chacked\u201d the loss function by learning a\ncollapsed distribution, while the prompted LLM actually shows substantial signs of ranking the inputs\ncorrectly.\nThis contrast can be reflected in a Concordance Correlation Coefficient (CCC) of 0.01for NeoBERT,\ncontrasting with a CCC of 0.69for detailed prompting. We thus suggest the use of the CCC as an\nadditional, and perhaps more appropriate, evaluation metric for RiR tasks.\nCCC measures both the correlation and agreement between predicted and ground truth values, defined\nas2\u03c1\u03c3y\u03c3\u02c6y\n\u03c32y+\u03c32\n\u02c6y+(\u00b5y\u2212\u00b5\u02c6y)2, where \u03c1is the Pearson correlation coefficient between predictions \u02c6yand ground\ntruth y,\u03c3yand\u03c3\u02c6yare their respective standard deviations, and \u00b5yand\u00b5\u02c6yare their means. CCC\npenalizes systematic bias and rewards predictions that maintain the natural variance of the true\ndistribution.\n2.2 D ETECTING MATHEMATICAL ERRORS\nStarting from ProcessBench (Zheng et al., 2024), we test the ability of models to predict the fraction\nof a mathematical solution up to the first erroneous reasoning step, given a problem and incorrect\nsolution in LaTeX. The goal of the system is to balance good precision in its individual predictions\nwhile also ranking the different errors correctly. To effectively do this, a model must systematically\napply logical rules rather than relying on probabilistic heuristics, but it must also be good at estimating\nrelative lengths and inferring the boundaries of the steps in a calibrated way. This would allow, for\ninstance, the resulting regression model to become a component of a continuous reward, e.g. to help\ntrain models to reason correctly over long sequences of steps or to build systems that can offer partial\ncredit for grading mathematics assignments.\nTo convert this originally discrete classification task into a regression problem, we first filter out\nproblems with correct solutions or final answers. We then merge all solution steps into a single\ncontinuous text T=s1\u2225s2\u2225\u00b7\u00b7\u00b7\u2225 sn(here\u2225denotes concatenation). Next, for a solution with error at\nstepk, the regression score Ris10\u00d7(k\u22121P\ni=1|s1|+1\n2|sk|)/|T|where|s1|denotes the length of step i,\nand|T|is the total length of the concatenated solution. See an example entry in Appendix D.\n2.3 P AIRWISE RAG C OMPARISON\nThis task centers on judging the quality of LLM outputs as a direct comparison between two candidates\nfor each different user query. We derive a dataset from the RAG-QA evaluations (Han et al., 2024).\nEach query q\u2208 Q has responses A1, A2and a target comparative score from \u22122to2representing\ntheaverage annotation of three human judges, who were instructed to assess response helpfulness,\ntruthfulness, and completeness. Here, positive scores means that A1is better (and vice versa). This\ntask partially aligns with RiR as judging the outputs and comparing them in light of each query often\nrequires nuanced judgment, especially to estimate a precise score while remaining calibrated.\n2.4 E SSAY GRADING\nWe lastly use an essay grading dataset (Crossley et al., 2023), where each entry contains among other\nfeatures an essay prompt, a student (grade 8\u201312) response, associated demographic information, and\nan overall score between 1and5. Essay Grading, the least complex task among these benchmarks,\ninvolves qualitative assessment of structured textual inputs. Models must assess various textual and\ncontextual features such as coherence, grammar, syntax, and thematic completeness. Although Essay\nGrading is simpler than the other tasks, it serves as a reference point for the other RiR tasks.\n3 MENTAT\nMENTAT combines two simple ideas. First, it allows the LLM itself to reflect, in batches, on any\npatterns in its prediction mistakes and uses that to incrementally adjust its own prompt. Second, it\naggregates multiple rollouts from the optimized LLM system with a simple trained MLP.\n4\n\n--- Page 5 ---\nBelow, we describe the two training phases of MENTAT : (1) iterative prompt evolution through mistake\nanalysis, (2) multi-rollout prediction with neural aggregation training. For a full description of the\nmethod see Figure 4.\nPhase 2:  Multi-Rollout Prediction w/ Neural Aggregation:  \n : number of iterations : number of rollouts : worst example count : history : weight : performance report Phase 1:  Iterative Prompt Evolution\nFigure 4: Phase 1 performs iterative prompt evolution by repeatedly evaluating candidate prompts,\nextracting worst-case examples, and refining prompts through error analysis, retaining the best prompt\nPbest. Phase 2 generates multi-rollout predictions by applying Pbestacross train/validation/test sets\nand aggregating Kstochastic predictions per input and trains a neural aggregator f\u03b8on sorted rollouts\nusing a combined CCC\u2013NMSE loss. The final test prediction is obtained by processing test rollouts\nand applying the trained aggregator f\u03b8\u22c6.\n3.1 P HASE 1: P ROMPT EVOLUTION\nThe first step of MENTAT is to make sure that the LLM prompt reflects any important local instructions\nfor how to reason about each individual instance or global guidance about the distribution of\nground-truth scores. In principle, any approach for prompt optimization can be used here, e.g.\nMIPRO (Opsahl-Ong et al., 2024) or GEPA (Agrawal et al., 2025). However, through preliminary\nexperiments, we identified two special properties in RiR tasks that call for different design choices.\nFirst, while many tasks can be achieved with well-optimized small models, we find that RiR tasks\ncan benefit from using the most powerful reasoning models available. In order to remain within the\nlightweight constraints of our RiR framework, the prompt evolution stage would have to minimize\nboth the number of rollouts performed with the LLM and the number of inherently sequential stages\nor iterations of optimization. Second, optimizing prompts for RiR tasks has the fairly unique property\nthat the patterns across examples are at least as important as the per-example error. This is because\nMENTAT \u2019s aggregation design demonstrates that it can be easy to turn a well-calibrated system into\none that has low pointwise error, but the reverse is not necessarily true.\nThese properties motivate us to test an exceedingly simple reasoning-based technique for optimizing\nLLM systems that contain a single prompt.1Unlike existing prompt optimizers, which typically seek\nto make improvements driven by individual failures (or very small batches of failures), MENTAT \u2019s\nprompt evolver is centered around asking the LLM to jointly reason about tens of mistakes at once.\nThis simple design is inspired by observing how human prompt engineers iterate on their systems in\npractice (Husain & Shankar, 2024).\n1We leave extending this method to multi-stage LLM programs and conducting an extensive comparison of\ndifferent prompt optimization strategies to future work.\n5\n\n--- Page 6 ---\nConcretely, we proceed in a very small number of sequential iterations (i.e., three in our experiments).\nIn each iteration, the work is highly parallelizable: we evaluate the current prompt on a shuffled\nsample of the training set, and then concatenate all of the rollouts for analysis by the same LLM. It is\nthen asked to identify systematic errors by analyzing the worst-performing examples and to generate\nimproved instructions. In each iteration, the LLM receives three key inputs: current instructions,\nperformance analysis with detailed error patterns, and a formatted history of previous optimization\nattempts. This historical context prevents the method from cycling through previously unsuccessful\napproaches and enables progressive refinement. At the end of this process, the best-performing\nprompt (via NMSE) is selected on a separate dev set.\nIn our evaluation, to stress MENTAT , we start from a deliberately basic prompt for each task, to reflect a\nmore challenging setting.2Note also that this iterative prompt evolution follows a single optimization\ntrajectory. In principle, MENTAT could employ multiple random restarts, which could be parallelized\nto explore diverse regions of the prompt space. However, we focus on single-trajectory optimization\nboth for computational efficiency and algorithmic simplicity.\n3.2 P HASE 2: M ULTI -ROLLOUT GENERATION WITH NEURAL AGGREGATION\nUsing the best LLM-discovered prompt from Phase 1, MENTAT generates multiple independent\npredictions for each example. The multi-rollout approach captures the inherent uncertainty in LLM\npredictions, as each rollout can reason independently, and provides richer signal for the subsequent\nneural aggregation phase. In practice, we set this to three rollouts per example. We train a small Multi-\nLayer Perceptron (MLP) to combine rollout predictions. The aggregator ensures order invariance by\nsorting rollout predictions, incorporates statistical features (mean, standard deviation, min, max), and\nis optimized for a combination of the CCC and NMSE loss functions. Overall, this method can be\nunderstood as a variant of self-consistency (Wang et al., 2023) or best-of- Nvoting (Stiennon et al.,\n2020; Snell et al., 2024), but it leverages more information across the rollouts.\n4 E VALUATION\nWe define two standard baselines for RiR problems: fine-tuning a small Transformer encoder and\nprompting an LLM, and use these two to understand the relative merits of our method MENTAT and to\ndevelop a series of ablation experiments.\n4.1 B ASELINE : FINETUNING A TRANSFORMER ENCODER\nWe formulate RiR as supervised regression using a 250M-parameter NeoBERT model. The archi-\ntecture processes minimally formatted text sequences (e.g., combining problem statements with\nsolutions for math errors, augmented with domain-specific prompts).\nInputs are tokenized using NeoBERT\u2019s byte-level BPE tokenizer, truncated or padded to 1024 tokens,\nand passed through the pretrained encoder. The model extracts representations from the [CLS]\ntoken, applies dropout regularization ( p= 0.2), and uses linear projection for scalar predictions.\nThe optimization objective minimizes weighted NMSE and CCC using AdamW (Loshchilov &\nHutter, 2019). This architecture requires only prompt templating beyond standard fine-tuning, with\nhyperparameters detailed in Appendix C.1.\n4.2 B ASELINE : PROMPTING A LARGE LANGUAGE MODEL\nWe employ Chain-of-Thought style prompting to encourage frozen two LLMs, one non-reasoning\n(gpt-4.1 ) and reasoning ( gpt-5 ) to perform explicit reasoning through step-by-step token generation.\nDetailed prompts guide decomposition of complex inputs (templates in Appendix E).\nThis approach is motivated by several practical advantages. Frozen LLMs can act as a unified\ninterface across various natural language tasks, with no or very little training data. This is especially\n2Examples of the basic vs. the detailed prompts used for the three tasks can be found in Appendices G and E),\nrespectively, differing in the inclusion of detailed procedural steps, calibration guidance, and/or domain-specific\nheuristics that human experts may decide to include.\n6\n\n--- Page 7 ---\nLM MethodMath Errors Pairwise RAG Essay Grading\nNMSE CCC NMSE CCC NMSE CCC\n100 500 100 500 100 500 100 500 100 500 100 500\nMain Methods\nNeoBERT Gradient Descent 1.05 1.01 0.02 0.06 1.44 1.02 0.01 0.10 1.03 0.91 0.19 0.65\nGPT-4.1 Basic Prompt 1.59 1.59 0.36 0.36 2.18 2.18 0.47 0.47 0.75 0.75 0.63 0.63\nDetailed Prompt 1.13 1.13 0.52 0.52 2.20 2.20 0.47 0.47 0.73 0.73 0.65 0.65\nMENTAT Basic Prompt 0.87 0.76 0.51 0.49 0.77 0.80 0.50 0.52 0.54 0.53 0.70 0.68\nGPT-5 Basic Prompt 0.77 0.77 0.66 0.66 2.25 2.25 0.35 0.35 1.31 1.31 0.42 0.42\nDetailed Prompt 0.78 0.78 0.69 0.69 2.18 2.18 0.31 0.31 1.53 1.53 0.40 0.40\nMENTAT Basic Prompt 0.52 0.42 0.72 0.78 1.07 0.93 0.36 0.33 0.64 0.67 0.59 0.55\nAblations\nGPT-4.1 MENTAT Prompt 1.39 1.29 0.45 0.48 2.00 1.69 0.45 0.48 0.61 0.71 0.68 0.66\nMENTAT -Avg 1.00 1.01 0.52 0.52 1.82 1.48 0.48 0.51 0.57 0.63 0.69 0.68\nGPT-5 MENTAT Prompt 0.66 0.58 0.66 0.72 1.43 1.95 0.33 0.30 0.74 0.70 0.57 0.54\nMENTAT -Avg 0.59 0.51 0.68 0.75 1.31 1.83 0.35 0.32 0.69 0.67 0.57 0.55\nTable 1: Performance comparison on three tasks using NMSE and CCC metrics (NMSE/CCC). Each\nentry is the average of three independent runs on a test set of size 750. Total training sizes are 100\nand500(train/val combined). Ablations: MENTAT Prompt uses only error-driven prompt refinement\non training data; MENTAT -Avg uses MENTAT prompting + multiple rollouts + averaging (instead of an\nMLP like in full MENTAT ). We remark here that NeoBERT obtains an average NMSE and CCC of\n0.60and0.66respectively, on a training regime of 1500 (1000 training + 500validation) on Pairwise\nRAG Comparison. That is, NeoBERT needs much more data on this task to lead to good performance,\nbut it can be achieved.\nvaluable in reasoning-intensive regression (RiR) tasks where annotated datasets are often scarce.\nUtilizing a shared, unified, and amortized infrastructure (i.e., LLM servers) enables us to deploy a\nsingle model across many tasks, significantly reducing the computational and financial overhead\ncompared to training multiple specifiable models.\n4.3 E XPERIMENTAL SETUP\nAll configurations use 750 test examples with results averaged across three independent runs. We\nevaluate all methods on two training configurations (100 and 500 samples), which are sized to reflect\nreal-world data constraints:\n\u2022Prompt Optimization Methods: Balanced splits of 50 + 50 and250 + 250 (train+validation).\n\u2022 Finetuning NeoBERT: Training-heavy splits of 50 + 50 and350 + 150 .\n4.4 R ESULTS\nOur main evaluation results are reported in Table 1, demonstrating significant performance variations\nacross methods and tasks. The results reveal distinct patterns in how different approaches handle\nreasoning-intensive regression problems, with MENTAT consistently outperforming baseline methods\nacross most configurations.\n4.4.1 M ATHEMATICAL ERROR DETECTION PERFORMANCE\nOn this task, finetuning NeoBERT achieves near-zero CCC scores across both training configurations\nand effectively collapsing to mean predictions as shown in Figure 1. In contrast, LLM-based ap-\nproaches demonstrated substantial reasoning capabilities. gpt-4.1 with detailed prompting achieved\nCCC scores of 0.52(100-sample training) and maintained this performance at 500 samples. However,\nMENTAT with gpt-4.1 showed only modest improvements, reaching CCC scores of 0.51(100 sam-\nples) and 0.49(500 samples), representing approximately stable performance with slight variation.\nWe hypothesize that gpt-4.1 \u2019s limited reasoning capabilities on this reasoning-intensive task made it\ndifficult to understand its own errors and thus improve.\n7\n\n--- Page 8 ---\nThe most dramatic improvements can be seen with gpt-5 . While detailed prompting with gpt-5\nachieved strong baseline performance (CCC: 0.69, NMSE: 0.78), MENTAT with gpt-5 delivered\nsubstantial enhancements. In the 100-sample training regime, CCC improved by 4.3%, while\nNMSE improved by 33.3%. In the 500-sample training regime, CCC improved by 13%, while\nNMSE improved by 46.2%. These results indicate that MENTAT \u2019s iterative prompt refinement and\nneural aggregation effectively leverage gpt-5 \u2019s reasoning capabilities while addressing the precision\nlimitations inherent in direct LLM numerical prediction.\n4.4.2 P AIRWISE RAG P ERFORMANCE AND GPT-5\nOn the pairwise RAG comparison task, finetuning NeoBERT achieved very low CCC scores while\nappearing competitive on the NMSE metric by \u201chacking\u201d the distribution. Surprisingly, gpt-4.1\ndemonstrated superior performance compared to gpt-5 on this task, in sharp contrast with the general\ntrend observed in mathematical error detection. Detailed prompting with gpt-4.1 achieved CCC\nscores of 0.47across both training configurations, while gpt-5 detailed prompting resulted in lower\nCCC scores of 0.31.\nUnlike math errors and essay grading tasks, correct decisions on the pairwise RAG benchmark often\nhinge on a few salient cues and short justifications. With chain-of-thought scaffolds on this task, we\nobserve that gpt-5 systematically \u201coverthinks,\u201d resulting in predictions that concentrate near the\ncenter ( 0on the [\u22122,2]margin) rather than faithfully spreading across the empirical label distribution.\nAs shown in Figure 5, its variance is under-dispersed relative to ground truth, with more than half of\nexamples yielding identical rollouts across three samples. Rollout correlations are very high, and the\nfinal numbers fall on a coarse grid (e.g., {\u22121,\u22121\n3,0,1\n3}), all consistent with hedging.\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0\nScore050100150200250300Frequency2\npred/2\ntrue=0.69\nGPT-5 vs Ground Truth\nGT mean: -0.12\nGPT-5 mean: -0.33\n2.0\n 1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5 2.0\nScoreFrequency2\npred/2\ntrue=1.75\nGPT-4.1 vs Ground Truth\nGT mean: -0.12\nGPT-4.1 mean: -0.17\nFigure 5: Pairwise RAG distributions on the mean of three rollouts vs. ground truth after the prompt\nevolution process. gpt-5 (left) is center-seeking and under-dispersed; gpt-5 (right) stays closer to\nthe empirical mean and exhibits greater spread. This behavior leads to higher CCC for gpt-4.1 .\nBy contrast, gpt-4.1 produces short, decisive judgments that remain closer to the dataset mean with\ngreater spread and more frequent use of the extremes. Although gpt-4.1 rollouts are also correlated,\nthe resulting distribution retains enough variance and calibration to yield substantially higher CCC.\nFor pairwise RAG, gpt-5 tends toward the center and compresses its numeric range, degrading\ndistributional fidelity (and thus CCC) even when NMSE remains similar.\nWe hypothesize that gpt-4.1 \u2019s superior performance over gpt-5 on pairwise RAG comparison\naligns with recent findings that large reasoning models often underperform on simpler tasks (Shojaee\net al., 2025). These models initially find correct solutions but continue reasoning toward incorrect\nanswers, suggesting that excessive sophisticated reasoning can sometimes be counterproductive. This\nhypothesis is supported by our observation that over half of gpt-5 \u2019s examples yield identical rollouts\nacross three samples, with final scores clustering on a coarse grid rather than reflecting the task\u2019s\ninherent variance.\n4.4.3 E SSAY GRADING PERFORMANCE\nEssay grading represented the least complex reasoning-intensive task, with NeoBERT achieving\nreasonable performance that improved substantially with additional training data. This aligns with\n8\n\n--- Page 9 ---\nthe task\u2019s characterization as requiring primarily semantic understanding rather than deep sequential\nreasoning. gpt-4.1 achieved strong baseline performance with detailed prompting (CCC: 0.65,\nNMSE: 0.73), while MENTAT provided meaningful improvements. In the 100-sample training regime,\nCCC improved by 7.7%and NMSE improved by 26.0%compared to detailed prompting. In the\n500-sample training regime, CCC improved by 4.6%and NMSE improved by 27.4%. Notably,\ngpt-5 performance on essay grading showed surprisingly poor concordance compared to gpt-4.1 ,\nsupporting the hypothesis discussed in Section 4.4.2 that sophisticated reasoning models may over-\ndeliberate on simpler tasks.\n5 C ONCLUSION\nWe investigated reasoning-intensive regression (RiR), where models must simultaneously tackle the\ntypical challenges of regression like precise predictions and proper ranking, while performing deep\nper-instance reasoning like identifying mathematical errors or using complex rubrics to compare text\noutputs. Our empirical findings reveal a tension in existing lightweight methods for tackling this\nproblem. Pure prompting leverages LLMs\u2019 reasoning capabilities but produces quantized, imprecise\noutputs. For instance, 87% of GPT predictions ended in .0 or .5 (see Figure 8). Conversely, supervised\nfinetuning for regression can sometimes even produce stronger NMSE scores by \u201chacking\u201d the loss\nfunction, avoiding risky predictions without capturing underlying reasoning patterns.\nWe proposed MENTAT , a simple method that suggests that hybrid approaches may help address\nthis tension through iteratively optimizing the prompts via batched error analysis combined with\nneural aggregation, achieving consistent improvements across our RiR tasks. However, we observe\nthat substantial headroom remains and hope that future work focus on developing more powerful\nreasoning-regression architectures that eliminate rather than merely mitigate the reasoning-precision\ntrade-off we observe and expand RiR to additional benchmarks across domains.\n6 L IMITATIONS\nDisagreements in Human Evaluations In the pairwise comparison case study, annotators are asked\nto explain why they prefer one answer over another, taking into account truthfulness, helpfulness, and\ncompleteness. Unfortunately, many of the answers in the dataset are likely unknown to the annotators\nbeforehand, limiting their ability to fully assess the responses. Therefore, this comparison dataset is\nnot about which answers are objectively superior, but rather which ones humans perceive as more\nhelpful, truthful, and complete. Although three human responses are collected per query to mitigate\nthis issue, it remains difficult to eliminate completely.\nRuntime Performance & Computational Costs In MENTAT, we sample multiple (in practice,\nthree) rollouts which are then fed to an MLP for training and at inference time. While these calls are\nall parallelizable, the increase in cost raises practical considerations about inference cost.\nACKNOWLEDGMENTS\nThis work used Expanse GPU at the San Diego Supercomputer Center (SDSC) through allocation\nCIS250733 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support\n(ACCESS; Boerner et al. 2023) program, which is supported by U.S. National Science Foundation\ngrants #2138259 ,#2138286 ,#2138307 ,#2137603 , and #2138296 . This research was partly\nsupported by Laude Institute.\nREFERENCES\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong,\nArnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik\nSen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, and Omar Khattab. Gepa:\nReflective prompt evolution can outperform reinforcement learning, 2025. URL https://arxiv.\norg/abs/2507.19457 .\n9\n\n--- Page 10 ---\nYash Akhauri, Bryan Lewandowski, Cheng-Hsi Lin, Adrian N. Reyes, Grant C. Forbes, Arissa Wong-\npanich, Bangding Yang, Mohamed S. Abdelfattah, Sagi Perel, and Xingyou Song. Performance\nprediction for large systems via text-to-text regression, 2025. URL https://arxiv.org/abs/\n2506.21718 .\nJames Urquhart Allingham, Jie Ren, Michael W. Dusenberry, Xiuye Gu, Yin Cui, Dustin Tran,\nJeremiah Zhe Liu, and Balaji Lakshminarayanan. A simple zero-shot prompt weighting technique\nto improve prompt ensembling in text-image models. In Proceedings of the 40th International\nConference on Machine Learning , ICML\u201923. JMLR.org, 2023.\nZachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, and Prithviraj Ammanabrolu.\nCritique-out-loud reward models. arXiv preprint arXiv:2408.11791 , 2024a.\nZachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, and Prithviraj Ammanabrolu.\nCritique-out-loud reward models, 2024b. URL https://arxiv.org/abs/2408.11791 .\nSimran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami,\nFrederic Sala, and Christopher R. Ask me anything: A simple strategy for prompting language\nmodels, 2022. URL https://arxiv.org/abs/2210.02441 .\nDaniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser, and Mikhail Belkin. Ag-\ngregate and conquer: detecting and steering llm concepts by combining nonlinear predictors over\nmultiple layers, 2025. URL https://arxiv.org/abs/2502.03708 .\nTimothy J. Boerner, Stephen Deems, Thomas R. Furlani, Shelley L. Knuth, and John Towns. ACCESS:\nAdvancing innovation: Nsfs advanced cyberinfrastructure coordination ecosystem: Services &\nsupport. In Practice and Experience in Advanced Research Computing (PEARC 23) , New York,\nNY , USA, 2023. Association for Computing Machinery. doi: 10.1145/3569951.3597559.\nL. Breiman. Stacked regressions. Machine Learning , 24:49\u201364, 1996a. URL https://api.\nsemanticscholar.org/CorpusID:27026167 .\nLeo Breiman. Bagging predictors. Mach. Learn. , 24(2):123140, August 1996b. ISSN 0885-6125.\ndoi: 10.1023/A:1018054314350. URL https://doi.org/10.1023/A:1018054314350 .\nLeo Breiman. Random forests. Mach. Learn. , 45(1):532, October 2001. ISSN 0885-6125. doi:\n10.1023/A:1010933404324. URL https://doi.org/10.1023/A:1010933404324 .\nLola Le Breton, Quentin Fournier, Mariam El Mezouar, John X. Morris, and Sarath Chandar. Neobert:\nA next-generation bert, 2025. URL https://arxiv.org/abs/2502.19587 .\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sashank Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra,\nKevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\nSaeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. Palm: scaling language modeling with pathways. J.\nMach. Learn. Res. , 24(1), March 2024. ISSN 1532-4435.\nScott A. Crossley, Yu Tian, Perpetual Baffour, Alex Franklin, Youngmeen Kim, Wesley Morris, Meg\nBenner, Aigner Picou, and Ulrich Boser. The english language learner insight, proficiency and\nskills evaluation (ellipse) corpus. International Journal of Learner Corpus Research , 2023. URL\nhttps://api.semanticscholar.org/CorpusID:267599728 .\nRbert Csords, Piotr Pikos, Kazuki Irie, and Jrgen Schmidhuber. Switchhead: Accelerating transform-\ners with mixture-of-experts attention, 2024. URL https://arxiv.org/abs/2312.07987 .\n10\n\n--- Page 11 ---\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\n2025. URL https://arxiv.org/abs/2501.12948 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019. URL https://arxiv.org/abs/\n1810.04805 .\nShahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. Ragas: Automated evaluation\nof retrieval augmented generation. In Proceedings of the 18th Conference of the European Chapter\nof the Association for Computational Linguistics: System Demonstrations , pp. 150\u2013158, 2024.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter\nmodels with simple and efficient sparsity. J. Mach. Learn. Res. , 23(1), January 2022. ISSN\n1532-4435.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr \u00b4e Martins, Graham Neubig,\nAnkush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leverag-\ning large language models for fine-grained machine translation evaluation. In Philipp Koehn, Barry\nHaddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine\nTranslation , pp. 1066\u20131083, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.wmt-1.100. URL https://aclanthology.org/2023.wmt-1.100/ .\nYoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Proceedings\nof the Thirteenth International Conference on International Conference on Machine Learning ,\nICML\u201996, pp. 148156, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers Inc. ISBN\n1558604197.\nM.A. Ganaie, Minghui Hu, A.K. Malik, M. Tanveer, and P.N. Suganthan. Ensemble deep learning: A\nreview. Engineering Applications of Artificial Intelligence , 115:105151, 2022. ISSN 0952-1976.\ndoi: https://doi.org/10.1016/j.engappai.2022.105151. URL https://www.sciencedirect.com/\nscience/article/pii/S095219762200269X .\nNate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are\nzero-shot time series forecasters, 2024. URL https://arxiv.org/abs/2310.07820 .\nRujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan\nMin, and Vittorio Castelli. Rag-qa arena: Evaluating domain robustness for long-form retrieval\naugmented question answering. 2024. URL https://arxiv.org/abs/2407.13998 .\nYi-Chong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, and Ting Liu.\nEnsemble learning for heterogeneous large language models with deep parallel collaboration. 2024.\nURL https://api.semanticscholar.org/CorpusID:269282634 .\nHamel Husain and Shreya Shankar. Ai evals for engineers & pms. https://maven.com/\nparlance-labs/evals , 2024. Cohort-based course on building and iterating evaluation sys-\ntems for AI products.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures\nof local experts. Neural Computation , 3(1):79\u201387, 1991. doi: 10.1162/neco.1991.3.1.79.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models\nwith pairwise ranking and generative fusion, 2023. URL https://arxiv.org/abs/2306.02561 .\nMingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, and\nJimmy Ba. Calibrating language models via augmented prompt ensembles. URL https://api.\nsemanticscholar.org/CorpusID:271797871 .\nM.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In Proceedings\nof 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan) , volume 2, pp.\n1339\u20131344 vol.2, 1993. doi: 10.1109/IJCNN.1993.716791.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Exploring\ndemonstration ensembling for in-context learning. In ICLR 2023 Workshop on Mathematical and\nEmpirical Understanding of Foundation Models , 2023.\n11\n\n--- Page 12 ---\nKimi Team. Kimi K2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 , 2025.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.\nMaking large language models better reasoners with step-aware verifier, 2023. URL https:\n//arxiv.org/abs/2206.02336 .\nTiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic\ntasks, 2023. URL https://arxiv.org/abs/2305.14201 .\nY . Liu and X. Yao. Ensemble learning via negative correlation. Neural Networks , 12(10):1399\u2013\n1404, 1999. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(99)00073-8. URL\nhttps://www.sciencedirect.com/science/article/pii/S0893608099000738 .\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https:\n//arxiv.org/abs/1711.05101 .\nKeming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou.\nRouting to the expert: Efficient reward-guided ensemble of large language models, 2023. URL\nhttps://arxiv.org/abs/2311.08692 .\nMichal Lukasik, Zhao Meng, Harikrishna Narasimhan, Aditya Krishna Menon, Yin-Wen Chang,\nFelix Yu, and Sanjiv Kumar. Better autoregressive regression with LLMs. In Submitted to\nThe Thirteenth International Conference on Learning Representations , 2024a. URL https:\n//openreview.net/forum?id=xGs7Ch3Vyo . under review.\nMichal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix Yu, and Sanjiv Kumar.\nRegression aware inference with LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen\n(eds.), Findings of the Association for Computational Linguistics: EMNLP 2024 , pp. 13667\u201313678,\nMiami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/\nv1/2024.findings-emnlp.799. URL https://aclanthology.org/2024.findings-emnlp.799 .\nBo Lv, Chen Tang, Yanan Zhang, Xin Liu, Ping Luo, and Yue Yu. URG: A unified ranking\nand generation method for ensembling language models. In Lun-Wei Ku, Andre Martins, and\nVivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024 , pp.\n4421\u20134434, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi:\n10.18653/v1/2024.findings-acl.261. URL https://aclanthology.org/2024.findings-acl.\n261/ .\nBo Lv, Chen Tang, Yanan Zhang, Xin Liu, Yue Yu, and Ping Luo. Specfuse: Ensembling large\nlanguage models via next-segment prediction, 2024b. URL https://arxiv.org/abs/2412.\n07380 .\nLindsay MacDonald. Generative ai use case: Using llms to score customer conversations, July 2024.\nURL https://www.montecarlodata.com/blog-generative-ai-use-case-assurance/ .\nMonte Carlo Data Blog.\nDakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato,\nJan-Philipp Frnken, Chelsea Finn, and Alon Albalak. Generative reward models, 2024. URL\nhttps://arxiv.org/abs/2410.12832 .\nCostas Mavromatis, Petros Karypis, and George Karypis. Pack of llms: Model fusion at test-time via\nperplexity optimization, 2024. URL https://arxiv.org/abs/2404.11531 .\nWilliam Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought,\n2024. URL https://arxiv.org/abs/2310.07923 .\nTung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi\nPerel, Yutian Chen, and Xingyou Song. Predicting from strings: Language model embeddings for\nbayesian optimization, 2024. URL https://arxiv.org/abs/2410.10190 .\nIsaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez,\nM Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data, 2024.\nURL https://arxiv.org/abs/2406.18665 .\n12\n\n--- Page 13 ---\nKrista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia,\nand Omar Khattab. Optimizing instructions and demonstrations for multi-stage language model\nprograms, 2024. URL https://arxiv.org/abs/2406.11695 .\nSungjin Park, Xiao Liu, Yeyun Gong, and Edward Choi. Ensembling large language models with\nprocess reward-guided tree search for better complex reasoning, 2024. URL https://arxiv.\norg/abs/2412.15797 .\nSilviu Pitis, Michael R. Zhang, Andrew Wang, and Jimmy Ba. Boosted prompt ensembles for large\nlanguage models, 2023. URL https://arxiv.org/abs/2304.05970 .\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and\nJeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\nURL https://arxiv.org/abs/1701.06538 .\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret\nZoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan\nLi, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-\nexperts meets instruction tuning:a winning combination for large language models, 2023. URL\nhttps://arxiv.org/abs/2305.14705 .\nParshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad\nFarajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning\nmodels via the lens of problem complexity, 2025. URL https://arxiv.org/abs/2506.06941 .\nChenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. Getting more out of\nmixture of language model reasoning experts, 2023. URL https://arxiv.org/abs/2305.14628 .\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally\ncan be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024.\nXingyou Song and Dara Bahri. Decoding-based regression, 2025. URL https://arxiv.org/abs/\n2501.19383 .\nXingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel, and Yutian Chen.\nOmnipred: Language models as universal regressors, 2024. URL https://arxiv.org/abs/2402.\n14547 .\nXingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel, and Yutian Chen.\nOmnipred: Language models as universal regressors, 2025. URL https://arxiv.org/abs/2402.\n14547 .\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learn-\ning Research , 15(56):1929\u20131958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.\nhtml .\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nneural information processing systems , 33:3008\u20133021, 2020.\nHongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu\nLiu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi\nChen, and Tao Yu. Bright: A realistic and challenging benchmark for reasoning-intensive retrieval,\n2025. URL https://arxiv.org/abs/2407.12883 .\nEric Tang, Bangding Yang, and Xingyou Song. Understanding llm embeddings for regression, 2024.\nURL https://arxiv.org/abs/2411.14708 .\nNandan Thakur, Ronak Pradeep, Shivani Upadhyay, Daniel Campos, Nick Craswell, and Jimmy\nLin. Support evaluation for the trec 2024 rag track: Comparing human versus llm judges. arXiv\npreprint arXiv:2504.15205 , 2025.\n13\n\n--- Page 14 ---\nChen Tianlong, Cheng Yu, Chen Beidi, Zhang Minjia, and Bansal Mohit. Mixture-of-experts in the\nera of llms: A new odyssey. ICML 2024 presentation slides, 2024. International Conference on\nMachine Learning (ICML).\nRobert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From words to numbers:\nYour large language model is secretly a capable regressor when given in-context examples, 2024.\nURL https://arxiv.org/abs/2404.07544 .\nPat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhang-\norodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm\ngenerations with a panel of diverse models, 2024. URL https://arxiv.org/abs/2404.18796 .\nHongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, and Mikhail Yurochkin.\nFusing models with complementary expertise, 2024a. URL https://arxiv.org/abs/2310.\n01542 .\nJunlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents\nenhances large language model capabilities. ArXiv , abs/2406.04692, 2024b. URL https:\n//api.semanticscholar.org/CorpusID:270357878 .\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n2023. URL https://arxiv.org/abs/2203.11171 .\nDavid H. Wolpert. Stacked generalization. Neural Networks , 5(2):241\u2013259, 1992. ISSN 0893-6080.\ndoi: https://doi.org/10.1016/S0893-6080(05)80023-1. URL https://www.sciencedirect.com/\nscience/article/pii/S0893608005800231 .\nChujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jin-\ngren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning,\n2024. URL https://arxiv.org/abs/2412.06559 .\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and\nMichael Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings\nof the 46th International ACM SIGIR Conference on Research and Development in Information\nRetrieval , SIGIR \u201923, pp. 23082313, New York, NY , USA, 2023. Association for Computing\nMachinery. ISBN 9781450394086. doi: 10.1145/3539618.3592047. URL https://doi.org/10.\n1145/3539618.3592047 .\nA E XTENDED RELATED WORK\nThis appendix presents more expansive related work besides those covered in the main sections.\nEnsemble Learning. Ensemble learning combines several individual models to obtain better per-\nformance (Ganaie et al., 2022). Classical methods include bagging, boostrapping, and stacking\n(Breiman, 1996b; Freund & Schapire, 1996; Wolpert, 1992; Breiman, 1996a). General methods\ninclude negative correlation learning, explicit/implicit ensembles, and homogeneous/heterogeneous\nensembles (Liu & Yao, 1999; Srivastava et al., 2014; Breiman, 2001). More recent ensembling ap-\nproaches for LLMs include LLM-Blender which seeks to pairwise compare from a set of Ndifferent\nLLMs to discern subtle differences in output, then merges the top Kranked outputs (Jiang et al.,\n2023). DeePEn (Huang et al., 2024) is an ensembling method in which probability distributions\nfrom individual LLMs are translated into a \u201crelative representation\u201d space (to bypass the vocabulary\ndiscrepancies), making aggregation possible. There are many recent works on fusion methods (Lv\net al., 2024a;b; Mavromatis et al., 2024; Park et al., 2024; Verga et al., 2024). (Wang et al., 2024a)\npropose a fusion-of-experts method which fuses outputs of multiple (expert) models with comple-\nmentary knowledge of the data distribution and casts it as a supervised learning problem. Prompt\nensembling has also had great success in improving task accuracy (Jiang et al.; Pitis et al., 2023;\nAllingham et al., 2023; Khalifa et al., 2023; Si et al., 2023; Arora et al., 2022; Li et al., 2023) along\nwith using Recursive Feature Machines (RFMs) for feature learning and aggregation for the steering\nof LLMs Beaglehole et al. (2025).\n14\n\n--- Page 15 ---\nRouting. Routing determines, from a pool of available LLMs, which model is best suited to produce\nthe most accurate and effective response to a given query. Recent work includes RouteLLM (Ong\net al., 2024), a framework for query routing between \u201cstrong\u201d and \u201cweak\u201d LLMs and Zooter (Lu\net al., 2023), a reward-guided routing approach that distills rewards from training queries into a\nrouting function, enabling precise allocation of each query to the LLM with the relevant expertise.\nMixture-of-Experts. Mixture-of-Experts (MoEs) is a framework in architecture design, in which\nmultiple specialized sub-models (\u201cexperts) handle different parts of the input space (Jacobs et al.,\n1991; Jordan & Jacobs, 1993; Shazeer et al., 2017). A gating mechanism then selects or weighs\nthese experts to generate a combined output. Recent work has sought to extend MoEs to LLMs,\nwhere several MLP experts are added after each multi-head self-attention module in the Transformer\nencoder and decoder blocks (Fedus et al., 2022; Chowdhery et al., 2024; Shen et al., 2023; Csords\net al., 2024). MoEs applications in LLMs have demonstrated demonstrated the ability to increase\nmodel size without a proportional rise in computational complexity, largely due to MoEs\u2019 inherently\nsparse computations (Tianlong et al., 2024). Recently, the mixture-of-agents (Wang et al., 2024b)\narchitecture has been proposed, in which multiple LLMs are stacked into sequential layers. Each\nlayers LLMs receive the responses from the previous layer for further refinement.\nNatural Language Regression. The two common approaches to solving natural language regression\nusing decoder-based LLMs includes autoregressive regression (Vacareanu et al., 2024; Lukasik et al.,\n2024b;a; Gruver et al., 2024; Liu & Low, 2023) and predictive head (Zhuang et al., 2023; Fernandes\net al., 2023). The former directly predicts the numerical target as text (e.g., predict 112by predicting\nthe tokens \u20181\u2019, \u20181\u2019, and \u20182\u2019). The latter approach learns a separate head on encoded inputs.\nCurrently, work on advancing regression tends to focus on non-reasoning classical feature-based\nregression tasks, this includes OmniPred (Song et al., 2024) which introduces a framework for\ntraining language models as universal end-to-end regressors. They train a 200M parameter T5\nencoder-decoder for the specific task of classical regression. Complementarily, (Nguyen et al., 2024)\nintroduces an \u201cembed-then-regress\u201d framework that leverages pre-trained language models\u2019 string\nembedding capabilities to map arbitrary text inputs into fixed-dimensional vectors for downstream\nregression.\nFine-tuning large language models (LLMs) represents a potential approach for RiR, but recent work\n(Lukasik et al., 2024b;a) studying conventional regression problems, generally without any reasoning,\ndemonstrates that decoder-only Transformers face fundamental optimization challenges for regression\ntasks due to the misalignment between cross-entropy loss (optimized for classification) and regression\nobjectives. Their work introduces Regression-Aware Fine-Tuning (RAFT), but demonstrates\u2014-on\nconventional regression tasks\u2014-only modest gains over encoder-only models like RoBERTa, despite\nrequiring extensive computational resources.\nOther recent work has explored specific language-oriented regression tasks that involve reasoning,\nparticularly for reward models in particular (Mahan et al., 2024; Ankner et al., 2024b). However, most\nsuch approaches rely on fine-tuning LLMs and extracting log-probabilities for special tokens at very\nlarge scale in terms of data and model size, since they tackle fairly general-purpose, one-time fitting\nof their models. In contrast, we are interested in particularly lightweight and data-efficient methods\nfor adapting LLMs to arbitrary reasoning-intensive regression problems with limited resources.\n15\n\n--- Page 16 ---\nB N UMERICAL OUTPUT QUANTIZATION IN LARGE LANGUAGE MODELS\nThe quantization patterns observed in LLM predictions demonstrate systematic precision limita-\ntions across reasoning-intensive regression tasks. Analysis of the test set per model on the math\nerrors task reveals that gpt-4.1 exhibits 63.1%clustering at .00/.50decimal endings, while gpt-5\nshows 59.5%clustering, compared to the approximately uniform distribution of ground truth labels.\nThis quantization bias appears consistently across both mathematical error detection and pairwise\nRAG comparison tasks, though the latter\u2019s discrete rating scale ([-2, 2]) somewhat constrains the\nrange of possible outputs. The observed clustering significantly deviates from uniform distribution\nexpectations, indicating systematic rather than random quantization behavior.\nThese findings highlight a fundamental challenge in direct LLM numerical prediction: while models\ncan perform sophisticated reasoning about regression problems, their text-based output generation\ninherently discretizes continuous values into a coarse grid. This quantization directly undermines\nregression precision requirements, particularly for tasks demanding fine-grained numerical discrimi-\nnation. The systematic nature of this bias across different model scales and tasks provides empirical\njustification for our neural aggregation approach, which leverages LLM reasoning capabilities while\ndelegating precise numerical prediction to conventional regression architectures better suited for\ncontinuous output generation.\n0 50 100 150 200 250 300 350\nFrequency.00.50.01-05.06-10.11-15.16-20.21-25.26-30.31-35.36-40.41-45.46-50.51-55.56-60.61-65.66-70.71-75.76-80.81-85.86-90.91-95.96-100Decimal Ending\n3722772171201015163251\n6123733333830344132432139424238423935414230GPT-5\nPredictions\nGround Truth\n0 100 200 300 400\nFrequency.00.50.01-05.06-10.11-15.16-20.21-25.26-30.31-35.36-40.41-45.46-50.51-55.56-60.61-65.66-70.71-75.76-80.81-85.86-90.91-95.96-100Decimal Ending\n43925338182101412\n6123733333830344132432139424238423935414230GPT-4.1\nPredictions\nGround TruthDecimal Ending Frequency Analysis - Math Errors\n.00 (Round) .50 (Half) .25/.75 (Quarter)\nFigure 6: Distribution of decimal endings in LLM numerical predictions versus ground truth labels\nfor mathematical error detection task ( n= 750 per distribution). gpt-4.1 predictions show 63.1%\nclustering at .00/.50endings ( 439 + 253 out of 750valid predictions), while gpt-5 shows 86.5%\nclustering ( 277+372 out of 750valid predictions). Ground truth labels exhibit approximately uniform\ndistribution across decimal ranges. This quantization bias demonstrates the systematic precision\nlimitations in direct LLM numerical output that necessitates our neural aggregation approach.\n16\n\n--- Page 17 ---\n0 100 200 300 400 500 600 700\nFrequency.00.50.16-20.21-25.26-30.31-35.36-40.66-70.71-75Decimal Ending\n69153312\n254362822023GPT-5\nPredictions\nGround Truth\n0 100 200 300 400 500 600 700\nFrequency.00.50.21-25.31-35.66-70.71-75Decimal Ending\n750254362822023GPT-4.1\nPredictions\nGround TruthDecimal Ending Frequency Analysis - Pairwise Comparison\n.00 (Round) .50 (Half) .25/.75 (Quarter)Figure 7: Distribution of decimal endings in LLM numerical predictions versus ground truth labels\nfor pairwise RAG comparison task ( n= 750 per distribution). gpt-4.1 predictions show 100%\nclustering at .00endings, while gpt-5 shows 99.2%clustering at .00/.50endings ( 691+53 out of 750\npredictions). The constrained [\u22122,2]rating scale with integer-like preferred values in ground truth\nlabels (primarily \u22122,\u22121,0,+1,+2) naturally limits decimal variation compared to the mathematical\nerror detection task. However, LLM predictions exhibit even more extreme quantization than the\nalready discrete ground truth distribution, with models defaulting almost exclusively to round integer\nvalues rather than utilizing the full continuous range available within the task\u2019s scoring rubric.\nC T RAINING THE MLP\nThe MLP model was trained using PyTorch with the following configuration and hyper-parameters:\n\u2022 Batch size: 32.\n\u2022 Number of epochs: 1000 .\n\u2022 Optimizer: AdamW with learning rate of 0.0001 .\n\u2022 Loss function: Weighted CCC and NMSE loss.\n\u2022 One hidden layer with dimension 8.\n\u2022 Training procedure: Mini-batch gradient descent with shuffled batches.\nThe model was trained with early stopping based on validation loss, monitoring at 100-epoch intervals.\nWe used the standard train/validation/test split ratios discussed in the experimental sections.\nMoreover, during training, both training and validation losses were monitored to ensure proper\nconvergence and avoid over-fitting. The model parameters corresponding to the best validation\nperformance were saved and used for final evaluation on the test set. This standardized training\nprocedure was used across all experiments, with the only variation being the input dimension size\nbased on the specific task configuration.\n17\n\n--- Page 18 ---\nC.1 NeoBERT\nThe implementation details (model parameters) for NeoBERT is below,\n\u2022No hidden layers; simple linear regression head that maps the 768-dimensional embedding\ndirectly to a single scalar\n\u2022 Optimizer: AdamW with default parameters.\n\u2022 Loss function: Weighted CCC and NMSE loss\n\u2022 batch size: 16\n\u2022 Training epochs: 10\nThe implementation used standard PyTorch Dataset and DataLoader classes for batching and GPU\nacceleration when available. All model weights were initialized from the pre\u2013trained NeoBERT -base\ncheckpoint except for the regression head, which used default PyTorch initialization.\n18\n\n--- Page 19 ---\nD E XAMPLE TASK ENTRIES\nD.1 M ATHEMATICAL ERROR DETECTION\nProblem:\nFind all ordered pairs (a, b)of positive integers such that 2a+ 1divides 3b\u22121and2b+ 1divides\n3a\u22121.\nSolution:\nTo find all ordered pairs (a, b)of positive integers such that 2a+ 1divides 3b\u22121and2b+ 1divides\n3a\u22121, we can start by setting up two equations based on the given conditions. Let\u2019s say 2a+ 1 = x\nand2b+ 1 = y. Then, we have: x|3b\u22121andy|3a\u22121. Since x= 2a+ 1andy= 2b+ 1, we know\nthat both xandyare odd numbers. Therefore we have 3b\u22121\u22610 (mod x) =\u21d23b\u22611 (mod x).\nMultiplying both sides by 2, we get 6b\u22612 (mod x). Since 6bis even, 2must also be even modulo\nx. But we know that xis an odd number, so this is only possible if 2\u22610 (mod x), which means\nxmust be a multiple of 2. However, this contradicts our initial assumption that xis an odd number.\nTherefore, there must be no solution for x, and consequently, no solution for (a, b). Hence, the only\npossible pair that might satisfy these conditions would be when both expressions are equal to each other\nand to 1. When 2a+ 1 = 1 and2b+ 1 = 1 ,a= 0andb= 0. However, since aandbmust be positive\nintegers, this solution does not meet the criteria. So, we will try another option. First, 2a+ 1 = 3 and\n2b+ 1 = 3 . Then a= 1andb= 1. Therefore, the only pair of positive integers satisfying the given\nconditions is (1,1).\nScore: 5.178\nFigure 8: Example entry the detecting mathematical errors dataset set. In this task, we are required to\ndetect where the initial error in a mathematical solution lies. The score indicates where the fraction\nof the solution that is correct (before the first error).\n19\n\n--- Page 20 ---\nE D ETAILED (HUMAN CRAFTED ) PROMPTS\nE.1 M ATHEMATICAL ERROR DETECTION\n1 \"\"\"\n2 You are a fair evaluator tasked with analyzing mathematical solutions and determining where the error\noccurs in the solution process .\n3\n4 Given a math problem and an incorrect solution . Analyze where the solution went wrong and assign a\nregression label from 0.0 to 10.0. :\n5 - 10.0 indicates the solution went wrong at the very end\n6 - 0.0 indicates the solution went wrong from the very beginning\n7 - Scores between 0.0 and 10.0 represent the fraction of the solution that was correct before the first\nerror . For example , 7.5 implies the first 75% of the solution was correct .\n8\n9 DO NOT PREDICT 10.0 or 0.0. The error occurs WITHIN the proposed solution .\n10 \"\"\"\nE.2 P AIRWISE RAG C OMPARISON\n1 \"\"\"\n2 You are a fair evaluator tasked with providing clear , objective feedback based on specific criteria ,\nensuring each assessment reflects the absolute standards set for performance .\n3\n4 A query ( likely a question ), a reference answer , the system generated answer , and a score rubric\nrepresenting evaluation criteria are given .\n5\n6 First , analyze step by step :\n7 1. Compare the system response to the reference answer in terms of helpfulness , truthfulness , and\ncompleteness\n8 2. Identify specific strengths and weaknesses of the system response\n9 3. Consider how well the system response addresses the query compared to the reference\n10\n11 Then provide your final score as a real number between -2.0 and 2.0. Remember :\n12\n13 2.0: The system generates a more comprehensive and accurate response that addresses the query better than\nthe reference answer in terms of helpfulness ( information that is relevant to answering the query ),\ntruthfulness ( information that is accurate and reliable ), and completeness ( the response covers all\naspects of the query ).\n14 1.0: The system generates a response that generally addresses the query and provides a satisfactory answer\nslightly better than the reference answer in terms of helpfulness , truthfulness , and completeness .\n15 0.0: The system generates a response that was as good as the reference answer in terms of helpfulness ,\ntruthfulness , and completeness .\n16 -1.0: The system generates a response that was slightly worse than the reference answer in terms of\nhelpfulness , truthfulness , and completeness .\n17 -2.0: The system generated a response that was noticably worse than the reference answer in terms of\nhelpfulness , truthfulness , and completeness .\n18 \"\"\"\nE.3 E SSAY GRADING\n1 \"\"\"\n2 You are an expert essay grader tasked with evaluating essay quality based on multiple features .\n3\n4 Analyze the essay systematically by considering :\n5 1. Text content grammar , vocabulary , and coherence\n6 2. Structural elements ( paragraphs , sentences , word count )\n7 3. Lexical diversity and vocabulary richness\n8 4. How well ideas flow and connect throughout the essay\n9 5. Overall organization\n10\n11 Use the demographic and linguistic features as supporting context , focusing on the cohesiveness , grammar ,\nand vobulary of the essay .\n12\n13 Provide your reasoning step by step , then assign a score between 1.0 -5.0 where :\n14 - 5.0: Highly cohesive with excellent flow , organization , vocabulary , and grammar\n15 - 3.0: Moderate cohesiveness with some unclear transitions . moderate vocabulary and grammar\n16 - 1.0: Very poor cohesiveness with little to no logical flow , poor vocabulary and grammar\n17 \"\"\"\n20\n\n--- Page 21 ---\nF LLM-D ISCOVERED PROMPTS\nF.1 M ATHEMATICAL ERROR DETECTION - GPT5\n1 \"\"\"\n2 Task : Identify the first objectively incorrect step in the provided solution and output a score in [0.0 ,\n10.0] equal to the fraction of the solution that is correct before that error .\n3\n4 Procedure :\n5 1) Segment into atomic steps :\n6 - Count each meaningful action : definitions / variable naming used later , correct restatements that\nconstrain the solution , substitutions , equation formations , identity applications , case / setup\nstatements , computations , and logical inferences .\n7 - Do not count pure fluff or repetition that does not affect the derivation .\n8\n9 2) Walk through in order and locate the first objectively incorrect item :\n10 - Wrong modeling / equation from the problem text (e.g., misreading k times m o r e ), wrong operation /\nunit handling , incorrect enumeration / listing in counting / probability , unjustified / arbitrary\nassumption when first used , invalid identity / application , or false deduction .\n11 - If a slip is immediately corrected and not used , do not treat it as the first error ; otherwise it is.\n12\n13 3) Casework / branches :\n14 - Count correct setup and any correct early branches before the flawed branch that is pursued to the\nconclusion .\n15 - The first error is the earliest false statement in the pursued path .\n16\n17 4) Determine the fraction :\n18 - Let T be the total number of counted steps .\n19 - Let k be the index (1- based ) of the first error ; the number of correct steps before the error is k 1 .\n20 - Fraction = ( k 1 )/T. If no error exists , fraction = 1.0.\n21\n22 5) Map to prediction :\n23 - Prediction = round (10 Fraction , 2) , bounded to [0.0 , 10.0].\n24 - Use fine granularity ; avoid anchoring to round numbers unless warranted by the step count .\n25\n26 Calibration reminders :\n27 - Early foundational mistakes ( modeling , first aggregation , first enumeration ) -> low scores (0 - 3).\n28 - Mid - solution errors ( within computation / casework ) -> mid scores ( 3 - 7).\n29 - Late slips after many valid steps ( final simplification , last identity ) -> high scores ( 7 - 10) .\n30\n31 Output only the numeric prediction .\n32 \"\"\"\nF.2 P AIRWISE RAG C OMPARISON - GPT5\n1 \"\"\"\n2 Scoring objective : Compare the system response to the reference answer along truthfulness , helpfulness ,\nand completeness , in that order of importance . Output a single score in [ -2.0 , 2.0]. Default to 0.0\nunless clear evidence moves the score .\n3\n4 Step -by - step :\n5 1) Identify the core question and the main claim (s) of the reference .\n6 2) Check alignment of the s y s t e m s main claim with the r e f e r e n c e s correct conclusions .\n7 - If the system contradicts a correct reference on the main point or introduces harmful misinformation :\n-1.5 to -2.0.\n8 - If partially correct but misses an important constraint / nuance : -0.33 to -1.0 depending on impact .\n9 3) Assess truthfulness of added details .\n10 - Reward only accurate , non - contradictory specifics . If details may be incorrect or conflict with the\nreference , subtract rather than add .\n11 4) Assess helpfulness / actionability and clarity .\n12 - Prefer concrete , targeted , and directly useful content over vague or generic advice .\n13 - Do not reward verbosity by itself .\n14 5) Assess completeness relative to the question .\n15 - Credit coverage of key aspects the reference missed , only if accurate and relevant .\n16\n17 Calibration guide ( avoid extremes unless warranted ):\n18 - +2.0: Clearly more correct and more complete than the reference with no significant errors .\n19 - +1.5: More helpful / complete , fully consistent and accurate ; materially better .\n20 - +1.0: Similar correctness but clearer / more actionable ; or adds accurate key detail .\n21 - +0.33 to +0.67: Slightly better in clarity or minor accurate additions .\n22 - 0.0: On par overall .\n23 - -0.33 to -0.67: Slightly worse ( minor inaccuracies , vagueness , or clarity issues ).\n24 - -1.0 to -1.5: Misses key point (s) or includes notable inaccuracies .\n25 - -2.0: Clearly incorrect on the main claim , misleading , or unsafe .\n26\n27 Additional safeguards :\n28 - Prioritize truthfulness over added breadth ; cap positive scores at +0.67 when added details are not\ncorroborated by the reference or are only marginally relevant .\n29 - When both answers reach the same correct conclusion , stay near neutral ; award modest positives only for\nclearly better clarity / actionability .\n30 - Use consistent , conservative scoring to reduce overuse of 2 .0.\n31\n32 \"\"\"\n21\n\n--- Page 22 ---\nF.2.1 E SSAY GRADING - GPT4.1\n1 \"\"\"\n2 Score essays holistically on a 1.0 5 .0 scale , prioritizing idea development and organization . Use these\nsteps and weights :\n3\n4 1) Purpose and Task Fulfillment (10%)\n5 - Identify the thesis / central claim and whether the essay addresses the prompt and maintains focus .\n6\n7 2) Development and Support (40%)\n8 - Assess specificity , relevance , and sufficiency of reasons / examples .\n9 - Reward concrete details , explanations , and sustained elaboration .\n10 - Do not require formal citations ; judge proportional to length .\n11\n12 3) Organization and Coherence (30%)\n13 - Check for clear introduction , body paragraphs with topic sentences , logical sequencing , transitions , and\na conclusion .\n14 - Reward multi - paragraph structure and logical flow even if language is non - native .\n15\n16 4) Language Use and Style (15%)\n17 - Consider clarity , sentence variety , and appropriate word choice .\n18 - Reward effective phrasing ; tolerate awkwardness if meaning is clear .\n19\n20 5) Mechanics (5%)\n21 - Penalize only when errors impede comprehension or severely disrupt flow .\n22 - Do not over - penalize non - native grammar , spelling , or minor errors .\n23\n24 Guardrails :\n25 - Do not use length , grade level , or vocabulary sophistication as direct proxies for quality . Length only\nmatters insofar as it enables development .\n26 - Redundancy / repetition reduces Development and Style modestly ; do not let it dominate the score .\n27 - Use the full 1.0 5 .0 range . Competent high - school argumentative / expository essays with clear thesis ,\ncoherent paragraphs , and relevant support typically fall in 3.5 4 .5 even with moderate grammar\nerrors .\n28\n29 Scale anchors :\n30 - 5.0: Exceptional clarity and control ; insightful development ; seamless organization ; errors , if any , are\ntrivial .\n31 - 4.0: Clear thesis ; coherent multi - paragraph structure ; solid , relevant support with some specificity ;\nminor lapses or noticeable but non - impeding errors .\n32 - 3.5: Adequate thesis and organization ; generally relevant support with limited depth or uneven\nelaboration ; errors present but meaning clear .\n33 - 3.0: Partially developed ; some organization but weak / uneven support or coherence ; frequent errors yet\noverall understandable .\n34 - 2.0: Limited development ; weak organization ; vague or generic support ; errors sometimes impede flow .\n35 - 1.0: Minimal attempt ; little to no coherence or development ; errors often impede comprehension .\n36\n37 Calibration tips :\n38 - If an essay has a clear stance , at least three coherent body paragraphs with topic sentences , logical\nprogression , and a conclusion , start at 3.8 and adjust 0 .5 for strength of support and clarity ; do\nnot drop below 3.0 unless coherence or comprehension breaks down .\n39 - Short but focused and coherent responses can score high if they present a clear thesis and well -\nconnected support proportional to length .\n40\n41 \"\"\"\n22\n\n--- Page 23 ---\nG B ASIC PROMPTS\nG.1 M ATHEMATICAL ERROR DETECTION\n1 \"\"\"\n2 Analyze the mathematical solution step by step and determine where the first error occurs . Then output a\nprediction between 0.0 and 10.0 , representing the fraction of the solution that was correct before\nthe error .\n3\n4 \"\"\"\nG.1.1 P AIRWISE RAG C OMPARISON\n1 \"\"\"\n2 Analyze the system response compared to the reference answer step by step . Consider helpfulness ,\ntruthfulness , and completeness . Then output score between -2.0 and 2.0 based on the rubric .\n3\n4 \"\"\"\nG.2 E SSAY GRADING\n1 \"\"\"\n2 Analyze the essay systematically by considering text content quality , structural elements , lexical\ndiversity , and how well ideas flow and connect throughout . Assign a score between 1.0 -5.0 ( with\n5.0 being the best ) based on overall quality .\n3\n4 \"\"\"\n23",
  "project_dir": "artifacts/projects/MENTAT_Reasoning_Intensive_Regression",
  "communication_dir": "artifacts/projects/MENTAT_Reasoning_Intensive_Regression/.agent_comm",
  "assigned_at": "2025-09-02T20:46:53.982239",
  "status": "assigned"
}